__author__ = "Alexandre Florio, Pedro Martins, Thiago Serra, and Thibaut Vidal"

import math
import random
from functools import reduce
from itertools import chain
from solution import Solution
import config as cfg

class Heuristic:
    """
    Builds a decision diagram using a top-down constructive heuristic with bottom-up pruning.

    Parameters
    ----------
    data : Dataset
        Dataset instance to be used for building the diagram.
    topology : Topology
        Topology instance to be used for building the diagram.
    randomize_splits : bool, default False
        If True, each split decision will be made using a randomly selected set of features, according
        to `feature_subset_ratio`.
    feature_subset_ratio : float, default 0.6
        Ratio of features to be used when `randomize_splits=True`.
    seed : int or None, default None
        Random seed to be used when `randomize_splits=True`.
    alpha : float, default 0.0
        Alpha hyperparameter.
    verbose : bool, default False
        Flag to turn on verbose logging.
    
    Attributes
    ----------
    solution : Solution
        The decision diagram Solution instance generated by the heuristic algorithm.
    """

    def __init__(self, data, topology, randomize_splits=False, 
                 feature_subset_ratio=0.6, seed=None, alpha=0.0, verbose=False):
        self.rand = random.Random(seed)
        
        self.verbose = verbose
        self.data = data
        self.topology = topology
        self.randomize_splits = randomize_splits
        self.alpha = alpha
        self.feature_subset_size = math.floor(len(data.features)*feature_subset_ratio)
        
        # The following dictionaries describe the decision diagram constructed in this method
        self.node_samples = { v: set() for v in self.topology.internal_nodes + self.topology.terminal_nodes }
        self.node_samples_per_parent = { v: {} for v in self.topology.internal_nodes + self.topology.terminal_nodes }
        self.node_feature = {}
        self.node_threshold = {}
        self.node_positive_arc = {}
        self.node_negative_arc = {}
        self.node_class = {}
        
        # We initialize the root node by assigning all samples to it
        self.node_samples[topology.root_node] = set([i for i in range(data.train_n)])

        self._build_top_down()
        self._prune_bottom_up()

    def _build_top_down(self):
        """Implements the top-down constructive algorithm"""
        # Fix each terminal node to its corresponding class
        for v in self.topology.nodes_per_layer[self.topology.layers-1]:
            index = v - (self.topology.internal_nodes[-1] + 1)
            self.node_class[v] = self.data.classes[index]
        
        # Layer by layer, we loop over all the non-terminal nodes to decide their branches and where they lead to
        for l in range(self.topology.layers-1):
            if self.verbose:
                print('Layer', l)

            # First we decide how to branch on each node of the layer
            for v in self.topology.nodes_per_layer[l]:
                if self.verbose: 
                    print("Deciding branch on",v)

                if len(self.node_samples[v]) == 0:
                    if self.verbose:
                        print(" Node is not reached by any prior arc")
                    continue

                total_samples_in_node = len(self.node_samples[v])

                if total_samples_in_node > 1:
                    # For each feature, we sort the samples by nondecreasing value and check every possible split
                    sorted_samples = list(self.node_samples[v].copy())
                    any_split_ever_found = False
                    best_split_cost, best_split_threshold, best_first_half, best_second_half, best_smallest_partition = [None]*5
                    if self.randomize_splits:
                        features = self.rand.sample(self.data.features, k=self.feature_subset_size)
                    else:
                        features = self.data.features
                    for f in features:
                        sorted_samples.sort(key = lambda i : self.data.train_X[i][f])
                        for idx in range(1,total_samples_in_node):
                            i = sorted_samples[idx-1]
                            j = sorted_samples[idx]
                            # We evaluate a split at each sample that has a greater value for the feature
                            if self.data.train_X[j][f] >= self.data.train_X[i][f] + cfg.epsilon:
                                (first_half,second_half) = self._initial_split_weighted_entropy(sorted_samples, idx)
                                split_cost = first_half + second_half
                                smallest_partition = min(idx, total_samples_in_node-idx)
                                # In case multiple splits have minimum cost, such as 0 if a split can effectively separate two classes, 
                                #  we choose the one that partitions the samples in larger sets to avoid nodes with very few samples
                                if best_split_cost is None or (best_split_cost > split_cost or (
                                    best_split_cost == split_cost and best_smallest_partition < smallest_partition)):

                                    any_split_ever_found = True
                                    best_split_cost = split_cost
                                    best_first_half = first_half
                                    best_second_half = second_half
                                    best_split_feature = f
                                    best_split_threshold = self.data.train_X[j][f]
                                    best_smallest_partition = smallest_partition
                                    
                    if any_split_ever_found:
                        self.node_feature[v] = best_split_feature
                        self.node_threshold[v] = best_split_threshold
                
                # If no split was found, we prune node v
                if total_samples_in_node <= 1 or not any_split_ever_found:
                    if self.verbose:
                        print(" No split found, pruning", v)
                    self._prune_node(v, update_solution=False)

                if self.verbose and v in self.node_threshold:
                    print(" Split cost:", best_first_half, "+", best_second_half, "=", best_split_cost)
                    print(" Split feature:", best_split_feature)
                    print(" Split threshold:", best_split_threshold)
                    print(" Split partitioning of samples:", best_smallest_partition, total_samples_in_node-best_smallest_partition)

            # Create ramification for each arc departing current layer
            next_layer_ramifications = []
            for v in self.topology.nodes_per_layer[l]:
                if v not in self.node_threshold:
                    continue
                positive_arc_samples = [i for i in self.node_samples[v] if self.data.train_X[i][self.node_feature[v]] >= self.node_threshold[v]]
                negative_arc_samples = [i for i in self.node_samples[v] if self.data.train_X[i][self.node_feature[v]] < self.node_threshold[v]]
                next_layer_ramifications.append(([(v,"+")], { v: positive_arc_samples }))
                next_layer_ramifications.append(([(v,"-")], { v: negative_arc_samples }))

            # In case there are more departing arcs than nodes in the next layer, merge destinations by decreasing cost
            if self.verbose:
                if len(next_layer_ramifications) > len(self.topology.nodes_per_layer[l+1]):
                    print("Merging ramifications into nodes of next layer")
            while len(next_layer_ramifications) > len(self.topology.nodes_per_layer[l+1]):
                ramifications = len(next_layer_ramifications)
                # We sort the ramifications by ascending number of samples assigned to each, 
                #  so that any ties when merging nodes (especially when the merging cost is 0
                #  because the samples of the merged ramifications have a single class) 
                #  will give preference to those assigned to fewer samples
                next_layer_ramifications.sort(key = lambda ram : len(ram[1]))

                # Select merge with best cost, while trying to avoid merging both arcs of a node
                best_merge_cost, best_valid_merge_r1, best_valid_merge_r2 = [None]*3
                best_invalid_merge_r1, best_invalid_merge_r2 = [None]*2
                found_valid_merge = False
                for r1 in range(ramifications):
                    for r2 in range(r1+1, ramifications):
                        r1_samples = list(chain(*next_layer_ramifications[r1][1].values()))
                        r2_samples = list(chain(*next_layer_ramifications[r2][1].values()))
                        current_pair_cost = self._list_weighted_entropy(r1_samples) + self._list_weighted_entropy(r2_samples)
                        merged_node_cost = self._list_weighted_entropy(r1_samples+r2_samples) 
                        merge_cost = merged_node_cost - current_pair_cost
                        if best_merge_cost is None or best_merge_cost > merge_cost:
                            best_merge_cost = merge_cost
                            if self._is_merge_valid(next_layer_ramifications[r1], next_layer_ramifications[r2]):
                                found_valid_merge = True
                                best_valid_merge_r1 = r1
                                best_valid_merge_r2 = r2
                            else:
                                best_invalid_merge_r1 = r1
                                best_invalid_merge_r2 = r2
                
                best_merge_r1 = best_valid_merge_r1 if found_valid_merge else best_invalid_merge_r1
                best_merge_r2 = best_valid_merge_r2 if found_valid_merge else best_invalid_merge_r2

                ram1, ram2 = next_layer_ramifications[best_merge_r1], next_layer_ramifications[best_merge_r2]
                ram1_samples, ram2_samples = list(chain(*ram1[1].values())), list(chain(*ram2[1].values()))
                if self.verbose:
                    print(" Merging", ram1[0], "with", len(ram1_samples), "samples and", ram2[0], "with", len(ram2_samples), "samples at cost", best_merge_cost)
                del next_layer_ramifications[best_merge_r2]
                del next_layer_ramifications[best_merge_r1]
                ram_samples_per_parent = { **ram1[1] }
                for k,v in ram2[1].items():
                    if k not in ram_samples_per_parent:
                        ram_samples_per_parent[k] = []
                    ram_samples_per_parent[k] += v
                next_layer_ramifications.append((ram1[0]+ram2[0], ram_samples_per_parent))

            # At this point, we have as many nodes in the next layer as ramifications, 
            #  so we just assign each to a node in the next layer
            ramification_number = 0
            for (ramification_arcs,samples_per_parent) in next_layer_ramifications:
                samples = list(chain(*samples_per_parent.values()))

                is_last_internal_layer = l == self.topology.layers-2
                samples_have_same_class = len(self._group_samples_in_classes(samples).keys()) == 1

                # If we are in the last internal layer or all samples are of the same class,
                # we want to assign the ramifications to the terminal node related to the samples' predominant class
                if is_last_internal_layer or samples_have_same_class:
                    predominant_class = self._predominant_class(samples)
                    v = self.topology.internal_nodes[-1] + (list(self.data.classes).index(predominant_class) + 1)
                else:
                    v = self.topology.nodes_per_layer[l+1][ramification_number]
                    ramification_number += 1
                
                self.node_samples[v].update(samples)

                for parent,parent_samples in samples_per_parent.items():
                    if parent not in self.node_samples_per_parent[v]:
                        self.node_samples_per_parent[v][parent] = set()
                    self.node_samples_per_parent[v][parent].update(parent_samples)

                if self.verbose:
                    print("Node",v,"has",len(samples),"samples and is reached by:")
                for (u,s) in ramification_arcs:
                    if self.verbose:
                        print(" ",u,s)
                    if s=="+":
                        self.node_positive_arc[u] = v
                    else:
                        self.node_negative_arc[u] = v

        self._generate_solution()

    def _prune_bottom_up(self):
        """Implements the bottom-up pruning algorithm"""
        if self.verbose:
            print("\nBottom-up pruning")
        pruning_gain = self.alpha / len(self.topology.internal_nodes)
        for l in reversed(range(1, self.topology.layers-1)):
            if self.verbose: print(f"Layer {l}")
            for v in self.topology.nodes_per_layer[l]:
                if v not in self.solution.used_nodes:
                    continue
                positive_arc = self.node_positive_arc[v]
                negative_arc = self.node_negative_arc[v]
                connects_to_leaves = positive_arc in self.topology.terminal_nodes or negative_arc in self.topology.terminal_nodes
                if not connects_to_leaves:
                    continue
                if self.verbose: print(f"Node {v}")
                accuracy = (self.data.train_n - self.solution.training_misclass) / self.data.train_n
                accuracy_if_pruned = (self.data.train_n - self._node_pruning_misclassification(v)) / self.data.train_n
                accuracy_cost = accuracy - accuracy_if_pruned
                if pruning_gain >= accuracy_cost:
                    if self.verbose: print("  Should prune")
                    self._prune_node(v)
                else:
                    if self.verbose: print("  Should not prune")

    def _generate_solution(self):
        """Sets a new Solution instance to the `solution` attribute"""
        self.solution = Solution(self.data, self.topology)
        self.solution.optimal = False
        self.solution.used_nodes.add(self.topology.root_node)
        for v in self.node_feature:
            self.solution.node_hyperplane[v] = [1 if f == self.node_feature[v] else 0 for f in self.data.features]
        for v in self.node_threshold:
            self.solution.node_intercept[v] = self.node_threshold[v]
        for v in self.node_positive_arc:
            w = self.node_positive_arc[v]
            self.solution.node_positive_arc[v] = w
            self.solution.used_nodes.add(w)
        for v in self.node_negative_arc:
            w = self.node_negative_arc[v]
            self.solution.node_negative_arc[v] = w
            self.solution.used_nodes.add(w)
        for v in self.node_class:
            self.solution.node_class[v] = self.node_class[v]
        self.solution.training_misclass = self._total_misclassified_samples()
    
    def _group_samples_in_classes(self, sample_list):
        """Based on the sample given as a list, constructs a dictionary with classes as keys and the sublist of samples belonging to each class"""
        samples_per_class = {}
        for i in sample_list:
            sample_class = self.data.train_Y[i]
            if sample_class not in samples_per_class:
                samples_per_class[sample_class] = [ i ]
            else:
                samples_per_class[sample_class].append( i )
        return samples_per_class

    def _list_weighted_entropy(self, sample_list):
        """Calculates the weighted entropy of the sample given as a list"""
        number_of_samples = len(sample_list)
        samples_per_class = self._group_samples_in_classes(sample_list)
        Shannon_entropy = 0
        for c in samples_per_class:
            probability = len(samples_per_class[c])/number_of_samples
            Shannon_entropy -= probability*math.log(probability)
        return number_of_samples*Shannon_entropy

    def _initial_split_weighted_entropy(self, sample_list, split_idx):
        """Calculates the total weighted entropy from partitioning a sample list at a given index"""
        first_half = self._list_weighted_entropy(sample_list[:split_idx]) 
        second_half = self._list_weighted_entropy(sample_list[split_idx:]) 
        return (first_half,second_half)

    def _node_pruning_misclassification(self, v):
        """Given a node `v`, calculates the resulting misclassification if the node was pruned"""
        samples_per_class = self._assigned_samples_per_class()
        for w in self.topology.terminal_nodes:
            if w not in self.solution.used_nodes:
                continue
            samples_per_class[self.node_class[w]].difference_update(self.node_samples[v])
        for (u,_) in self._parent_arcs(v):
            reassigned_samples = self.node_samples_per_parent[v][u]
            predominant_class = self._predominant_class(reassigned_samples)
            if predominant_class in samples_per_class:
                samples_per_class[predominant_class].update(reassigned_samples)
            else:
                samples_per_class[predominant_class] = set(reassigned_samples)
        return self._misclassified_samples(samples_per_class)

    def _prune_node(self, v, update_solution=True):
        """Prunes a node `v` from the decision diagram, removing it from the decision diagram and reassigning its sample flow"""
        for w in self.topology.terminal_nodes:
            self.node_samples[w].difference_update(self.node_samples[v])
            if v in self.node_samples_per_parent[w]:
                del self.node_samples_per_parent[w][v]
        for (u,s) in self._parent_arcs(v):
            reassigned_samples = self.node_samples_per_parent[v][u]
            predominant_class = self._predominant_class(reassigned_samples)
            w = { value: key for key,value in self.node_class.items() }[predominant_class]
            self._reassign_flow(u, w, s, reassigned_samples)
            
        if v in self.node_samples: self.node_samples[v] = set()
        if v in self.node_samples_per_parent: self.node_samples_per_parent[v] = {}
        if v in self.node_positive_arc: del self.node_positive_arc[v]
        if v in self.node_negative_arc: del self.node_negative_arc[v]
        
        if update_solution: self._generate_solution()

    def _parent_arcs(self, v):
        """Given a node `v`, yields all pairs `(u,s)` of its parent nodes and respective arc sides (+ or -)"""
        pos_arc_parents = [(u,'+') for u,w in self.node_positive_arc.items() if w == v]
        neg_arc_parents = [(u,'-') for u,w in self.node_negative_arc.items() if w == v]
        for (u,s) in chain(pos_arc_parents, neg_arc_parents):
            yield u,s
    
    def _reassign_flow(self, parent, child, side, samples):
        """Given a parent and child nodes, a side (+ or -) and a list of samples, assign the samples from parent to child"""
        self.node_samples[child].update(samples)
        if parent not in self.node_samples_per_parent[child]:
            self.node_samples_per_parent[child][parent] = set() 
        self.node_samples_per_parent[child][parent].update(samples)

        if side == '+':
            self.node_positive_arc[parent] = child
            if self.verbose: print("  Incoming positive arc from", parent, "now points to", child)
        else:
            self.node_negative_arc[parent] = child
            if self.verbose: print("  Incoming negative arc from", parent, "now points to", child)
    
    def _assigned_samples_per_class(self):
        """Returns a dictionary where keys are data set classes and values are lists of samples assigned to the respective class"""
        return { c: set(self.node_samples[node]) for node,c in self.node_class.items() if node in self.solution.used_nodes }
    
    def _total_misclassified_samples(self):
        """Calculates current misclassification"""
        return self._misclassified_samples(self._assigned_samples_per_class())

    def _misclassified_samples(self, samples_per_class):
        """Given a dictionary of samples per class, calculates the number of misclassified samples"""
        return len([sample for c in samples_per_class.keys() for sample in samples_per_class[c] if self.data.train_Y[sample] != c])

    def _predominant_class(self, samples):
        """Given a list of samples, returns which class most samples belong to"""
        classes = [self.data.train_Y[x] for x in samples]
        classes_count = reduce(lambda acc, x: { **acc, x: acc[x] + 1 if x in acc else 1 }, classes, {})
        predominant_class = -1
        best_count = -1
        for k,value in classes_count.items():
            if value > best_count:
                predominant_class = k
                best_count = value
        return predominant_class

    def _is_merge_valid(self, ram_a, ram_b):
        """Given two ramifications, determines if merging them would merge both arcs (+ and -) of some node"""
        ram_a_nodes = [item[0] for item in ram_a[0]]
        ram_b_nodes = [item[0] for item in ram_b[0]]
        for node in ram_a_nodes:
            if node in ram_b_nodes:
                return False
        return True